{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up a Spark Streaming context, processes the streamed data, and then trains a logistic regression model on the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "        \n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "        \n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Now we make a streaming thread and save the RDD files to a specified PATH variable.\n",
    "ssc = StreamingContext(sc, 30)\n",
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()\n",
    "ssc_t.stop()\n",
    "\n",
    "lines = ssc.socketTextStream(\"localhost\", 8080)\n",
    "PATH = ...\n",
    "lines.saveAsTextFiles(PATH)\n",
    "\n",
    "# 3.3 Reading in data and training model\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import string\n",
    "\n",
    "# Constructing the data frame\n",
    "df = pd.DataFrame([])\n",
    "for path, dir, files in os.walk(PATH):\n",
    "    for file in files:\n",
    "        if file.startswith('part'):\n",
    "            subsubfiles = os.path.join(path, file)\n",
    "            read = spark.read.json(subsubfiles)\n",
    "            pandasDF = read.toPandas()\n",
    "            df = pd.concat([df, pandasDF])\n",
    "\n",
    "df[\"messagesplit\"] = df[\"message\"].str.lower().str.split(' ')\n",
    "df[\"label\"] = (df[\"channel\"] == \"#jinnytty\").astype(float)\n",
    "\n",
    "# Removing punctuation\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "df_messagesplit2 = [list(map(lambda w: w.translate(table), comment)) for comment in df[\"messagesplit\"]]\n",
    "df[\"messagesplit\"] = df_messagesplit2\n",
    "\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "sc2 = SparkSession.builder.getOrCreate()\n",
    "spark_df = sc2.createDataFrame(df)\n",
    "\n",
    "# Splitting the dataset\n",
    "training, test = spark_df.randomSplit([0.8, 0.2], seed=4)\n",
    "\n",
    "# Data preprocessing and training with logistic regression\n",
    "from pyspark.ml.feature import Word2Vec, VectorAssembler, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=300, minCount=5, inputCol=\"messagesplit\", outputCol=\"result\")\n",
    "stringIndexer = StringIndexer(inputCol=\"username\", outputCol=\"username_StringIndexer\", handleInvalid=\"skip\")\n",
    "ohe = OneHotEncoder(inputCols=[\"username_StringIndexer\"], outputCols=[\"username_OneHotEncoder\"])\n",
    "assemblerInput = [\"result\", \"username_OneHotEncoder\"]\n",
    "vector_assembler = VectorAssembler(inputCols=assemblerInput, outputCol=\"VectorAssembler_Features\")\n",
    "\n",
    "stages = [word2Vec, stringIndexer, ohe, vector_assembler]\n",
    "pipeline = Pipeline().setStages(stages)\n",
    "model = pipeline.fit(training)\n",
    "training_df = model.transform(training)\n",
    "\n",
    "data = training_df.select(F.col(\"VectorAssembler_Features\").alias(\"features\"), F.col(\"label\"))\n",
    "lrModel = LogisticRegression().fit(data)\n",
    "\n",
    "# Training a model purely on the word2vec column\n",
    "assemblerInput2 = [\"result\"]\n",
    "vector_assembler2 = VectorAssembler(inputCols=assemblerInput2, outputCol=\"VectorAssembler_Features\")\n",
    "\n",
    "stages2 = [word2Vec, vector_assembler2]\n",
    "pipeline2 = Pipeline().setStages(stages2)\n",
    "model2 = pipeline2.fit(training)\n",
    "training_df2 = model2.transform(training)\n",
    "\n",
    "data2 = training_df2.select(F.col(\"VectorAssembler_Features\").alias(\"features\"), F.col(\"label\"))\n",
    "lrModel2 = LogisticRegression().fit(data2)\n",
    "\n",
    "# 3.4 Deploying the model on stream\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# The operations we applied to the training data, we also apply on the streamed data.\n",
    "# Note that the process function also includes code to print a confusion matrix.\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    print(\"========= %s =========\" % str(time))  # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df = df.toPandas()\n",
    "    df[\"messagesplit\"] = df[\"message\"].str.lower()\n",
    "    df[\"messagesplit\"] = df[\"messagesplit\"].str.split(' ')\n",
    "    df[\"label\"] = (df[\"channel\"] == \"#jinnytty\").astype(float)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    df_messagesplit2 = list()\n",
    "    for comment in list(df[\"messagesplit\"]):\n",
    "        stripped = [w.translate(table) for w in comment]\n",
    "        df_messagesplit2.append(stripped)\n",
    "    df[\"messagesplit\"] = df_messagesplit2\n",
    "    sc_pro = SparkSession.builder.getOrCreate()\n",
    "    spark_df_pro = sc_pro.createDataFrame(df)\n",
    "    # trained model is applied to the streamed data\n",
    "    pp_df = model.transform(spark_df_pro)\n",
    "    testData = pp_df.select(F.col(\"VectorAssembler_Features\").alias(\"features\"), F.col(\"label\"))\n",
    "    predictions = lrModel.transform(testData)\n",
    "    preds_and_labels = predictions.select(F.col(\"label\").cast(FloatType()), F.col(\"prediction\"))\n",
    "    preds_and_labels.show()\n",
    "    preds_and_labels = preds_and_labels.select(['prediction', 'label'])\n",
    "    metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "    print(metrics.confusionMatrix().toArray())\n",
    "\n",
    "# Because we want to see the model work without usernames, \n",
    "# we create a process based on the model trained purely on the word2vec column:\n",
    "\n",
    "def process2(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    print(\"========= %s =========\" % str(time))  # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df = df.toPandas()\n",
    "    df[\"messagesplit\"] = df[\"message\"].str.lower()\n",
    "    df[\"messagesplit\"] = df[\"messagesplit\"].str.split(' ')\n",
    "    df[\"label\"] = (df[\"channel\"] == \"#jinnytty\").astype(float)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    df_messagesplit2 = list()\n",
    "    for comment in list(df[\"messagesplit\"]):\n",
    "        stripped = [w.translate(table) for w in comment]\n",
    "        df_messagesplit2.append(stripped)\n",
    "    df[\"messagesplit\"] = df_messagesplit2\n",
    "    sc_pro = SparkSession.builder.getOrCreate()\n",
    "    spark_df_pro = sc_pro.createDataFrame(df)\n",
    "    # trained model is applied to the streamed data\n",
    "    pp_df = model2.transform(spark_df_pro)\n",
    "    testData = pp_df.select(F.col(\"VectorAssembler_Features\").alias(\"features\"), F.col(\"label\"))\n",
    "    predictions = lrModel2.transform(testData)\n",
    "    preds_and_labels = predictions.select(F.col(\"label\").cast(FloatType()), F.col(\"prediction\"))\n",
    "    preds_and_labels.show()\n",
    "    preds_and_labels = preds_and_labels.select(['prediction', 'label'])\n",
    "    metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "    print(metrics.confusionMatrix().toArray())\n",
    "\n",
    "# Starting the streaming thread:\n",
    "ssc = StreamingContext(sc, 10)\n",
    "ssc_t = StreamingThread(ssc)\n",
    "lines = ssc.socketTextStream(\"localhost\", 8080)\n",
    "lines.foreachRDD(process)\n",
    "\n",
    "# 3.4.1 Prediction with usernames\n",
    "ssc_t.start()\n",
    "\n",
    "\n",
    "\n",
    "ssc_t.stop()\n",
    "\n",
    "# 3.4.2 Prediction without usernames\n",
    "ssc2 = StreamingContext(sc, 10)\n",
    "ssc_t2 = StreamingThread(ssc2)\n",
    "lines2 = ssc2.socketTextStream(\"localhost\", 8080)\n",
    "lines2.foreachRDD(process2)\n",
    "ssc_t2.start()\n",
    "\n",
    "\n",
    "\n",
    "ssc_t2.stop()\n",
    "\n",
    "\n",
    "# Transforming the test data using model2\n",
    "test_pred_df = model2.transform(test)\n",
    "\n",
    "# Preparing the test data\n",
    "testdata = test_pred_df.select(\n",
    "    F.col(\"VectorAssembler_Features\").alias(\"features\"),  # renaming the column to \"features\"\n",
    "    F.col(\"label\")\n",
    ")\n",
    "\n",
    "# Making predictions using the logistic regression model\n",
    "predictions = lrModel2.transform(testdata)\n",
    "\n",
    "# Importing necessary libraries\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "\n",
    "# Creating a window specification\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "# Joining the message column with prediction and label by row number\n",
    "preds_labels = predictions.select(\n",
    "    F.col(\"label\").cast(FloatType()),\n",
    "    F.col(\"prediction\")\n",
    ").withColumn(\"columnindex\", row_number().over(w))\n",
    "\n",
    "message = test.select(\"message\").withColumn(\"columnindex\", row_number().over(w))\n",
    "\n",
    "preds_labels_message = preds_labels.join(\n",
    "    message, preds_labels.columnindex == message.columnindex, 'inner'\n",
    ").drop(message.columnindex)\n",
    "\n",
    "preds_labels_message.show()\n",
    "\n",
    "# Filtering messages where label and prediction both are 1\n",
    "filtered_messages_1 = preds_labels_message.filter(\n",
    "    (preds_labels_message.label + preds_labels_message.prediction) > 1\n",
    ").collect()\n",
    "\n",
    "# Filtering messages where label and prediction both are 0\n",
    "filtered_messages_0 = preds_labels_message.filter(\n",
    "    (preds_labels_message.label + preds_labels_message.prediction) < 1\n",
    ").collect()\n",
    "\n",
    "# Filtering messages where label is 1 but prediction is 0\n",
    "wrong_predictions_1 = preds_labels_message.filter(\n",
    "    (preds_labels_message.label == 1.0) & (preds_labels_message.prediction == 0.0)\n",
    ").collect()\n",
    "\n",
    "# Filtering messages where label is 0 but prediction is 1\n",
    "wrong_predictions_0 = preds_labels_message.filter(\n",
    "    (preds_labels_message.label == 0.0) & (preds_labels_message.prediction == 1.0)\n",
    ").collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
